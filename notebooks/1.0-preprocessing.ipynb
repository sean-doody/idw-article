{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## r/IntellectualDarkWeb Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import html\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Data Loader\n",
    "- Pulls `comments` and `posts` from SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str, tables: list) -> pd.DataFrame:\n",
    "    data = tuple()\n",
    "\n",
    "    for table in tables:\n",
    "        print(f'Loading {table}')\n",
    "        SQL = f'SELECT * FROM {table}'\n",
    "        conn = sqlite3.connect(path)\n",
    "        df = pd.read_sql(SQL, conn)\n",
    "        conn.close()\n",
    "        data = data + (df,)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data:**\n",
    "- Tables: `(comments, posts)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = ['comments', 'posts']\n",
    "db_path = os.path.join('..', 'data', 'sqlite', 'idw_reddit.db')\n",
    "comments, posts = load_data(path=db_path, tables=tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'N comments: {len(comments)}')\n",
    "print(f'N posts: {len(posts)}')\n",
    "print(f'Total: {len(comments) + len(posts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal text cleaner:\n",
    "- Normalizes text.\n",
    "- Removes noise from Reddit.\n",
    "- Removes HTML/XML/markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    '''\n",
    "    1. Removes noise specific to Reddit.\n",
    "    2. Unescapes and cleans HTML/XML.\n",
    "    3. Pulls out any text between anchor tags.\n",
    "    4. Removes URLs.\n",
    "    5. Removes leftover newlines (\\n)\n",
    "    6. Removes excess whitespace.\n",
    "    '''\n",
    "\n",
    "    # remove Reddit poll text\n",
    "    polls = re.compile(r'\\[View Poll\\]\\(\\S+\\)')\n",
    "    text = re.sub(polls, ' ', text)\n",
    "\n",
    "    # remove non-printable ASCII characters\n",
    "    non_print = re.compile(r'[^\\x20-\\x7E]')\n",
    "    text = re.sub(non_print, ' ', text)\n",
    "    \n",
    "    # remove 'submission statement' text\n",
    "    sub_statement = re.compile(r'submission statement', re.IGNORECASE)\n",
    "    text = re.sub(sub_statement, ' ', text)\n",
    "   \n",
    "    # clean HTML\n",
    "    text = markdown.markdown(text)\n",
    "    text = html.unescape(text)\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = ' '.join(soup.findAll(string=True))\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    # remove URLs\n",
    "    url_regex = re.compile(r'http\\S+|www\\S+|https\\S+')\n",
    "    text =  re.sub(url_regex, ' ', text)\n",
    "    \n",
    "    # remove excess white space\n",
    "    text = ' '.join(text.strip().split())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer functions:\n",
    "- Use the `all-mpnet-base-v2` tokenizer to count the tokens.\n",
    "- If > `382` tokens, tokens must be split into batches of lenght `n=382`.\n",
    "  - `all-mpnet-base-v2` accepts sequences of length `384`.\n",
    "  - But we have to account for special tokens `<s>` and `</s>` at the beginning and end of each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate tokenizer:\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_processor(text: str) -> tuple:\n",
    "    max_tokens = 382\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    n_tokens = len(tokens)\n",
    "    \n",
    "    if n_tokens > max_tokens:\n",
    "        tokens = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "    \n",
    "    return n_tokens, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode lists of texts to strings:\n",
    "- This function will be used to expldoe the dataframe observations into multiple rows.\n",
    "- A post or comment will only have repeating observations when it has been broken into sublists due to it having > `382` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify_lists(text: list) -> list:\n",
    "    if isinstance(text[0], list):\n",
    "        joined_texts = [' '.join(sublist) for sublist in text]\n",
    "    else:\n",
    "        joined_texts = [' '.join(text)]\n",
    "    return joined_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Posts\n",
    "- A couple things:\n",
    "  - remove titles in the set `['[deleted by user]', '[ Removed by Reddit ]', 'test']`\n",
    "    - These will not have any content.\n",
    "  - Remove posts by username `######` (anonymized for public view)\n",
    "    - This account spammed the same post over and over again\n",
    "- If a post has a title but no selftext body, use the title as the representation.\n",
    "  - This can be the case with selftext posts set to `[deleted]` or `[removed]` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_titles = ['[deleted by user]', '[ Removed by Reddit ]', 'test']\n",
    "remove_users = ['######'] # anonymized for public view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = posts.loc[~posts['title'].isin(remove_titles)].copy()\n",
    "posts = posts.loc[~posts['author'].isin(remove_users)].copy()\n",
    "print(f'N={len(posts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean text:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['clean_title'] = posts['title'].progress_apply(clean_text)\n",
    "posts['clean_body'] = posts['selftext'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace `[deleted]` and `[removed]` with empty strings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.loc[posts['clean_body'].isin(['[deleted]', '[removed]']), 'clean_body'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenate post titles and post bodies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['text_representation'] = posts['clean_title'] + ' ' + posts['clean_body']\n",
    "posts['text_representation'] = posts['text_representation'].progress_apply(lambda row: ' '.join(row.strip().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['text_representation'].sample(n=10).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grab token counts and tokenized lists:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vals = Parallel(n_jobs=mp.cpu_count()-1)(delayed(tokenizer_processor)(doc) for doc in tqdm(posts['text_representation'].tolist()))\n",
    "posts['n_tokens'], posts['tokens'] = zip(*token_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get processed texts and full_ids:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_processed = posts[['full_id',  'text_representation', 'tokens', 'n_tokens']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_processed['tokens'].sample(n=3).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove any comments that are simply: `['[deleted]', '[removed]', '[ Removed by Reddit ]']`\n",
    "- Remove comments from known bots: `['######', '######']` (anonymized for public view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_comments = ['[deleted]', '[removed]', '[ Removed by Reddit ]']\n",
    "remove_comment_users = ['######', '######'] # anonymized for public view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.loc[~comments['body'].isin(remove_comments)].copy()\n",
    "comments = comments.loc[~comments['author'].isin(remove_comment_users)].copy()\n",
    "len(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean text:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['text_representation'] = Parallel(n_jobs=mp.cpu_count()-1)(delayed(clean_text)(doc) for doc in tqdm(comments['body'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['text_representation'].sample(n=10).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grab token counts and tokenized lists:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vals = Parallel(n_jobs=mp.cpu_count()-1)(delayed(tokenizer_processor)(doc) for doc in tqdm(comments['text_representation'].tolist()))\n",
    "comments['n_tokens'], comments['tokens'] = zip(*token_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get processed texts and full_ids:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_processed = comments[['full_id', 'text_representation', 'tokens', 'n_tokens']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['tokens'].sample(n=1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Dataframes\n",
    "- Combine `posts_processed` and `comments_processed` into a single dataframe.\n",
    "- Ensure only unique `full_id`s are present.\n",
    "- Only keep posts/comments with a minimum of `10` tokens.\n",
    "  - This helps eliminate short posts/comments that may not contain rich topical content.\n",
    "- Save text representations.\n",
    "  -  The `text_representation` column will be used for extracting topic keywords.\n",
    "  - The `tokens` column will be used for creating embeddings and clustering.\n",
    "- Then, explode the lists of tokens.\n",
    "  - First, call the `stringify_lists` function.\n",
    "  - This is necessary for when multiple observations occur for a single `full_id`.\n",
    "  - Multiple observations occur when a post/comment was batched into sublists when their token count > `382`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_processed['source'] = 'post'\n",
    "comments_processed['source'] = 'comment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.concat(\n",
    "    [\n",
    "        posts_processed,\n",
    "        comments_processed\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(training_data) == len(training_data['full_id'].unique())\n",
    "print(f'Unique posts/comments: {len(training_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter tokens:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_minimum = 10\n",
    "training_data = training_data.loc[training_data['n_tokens'] >= token_minimum].copy()\n",
    "training_data['n_tokens'].min(), training_data['n_tokens'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save text representations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_reps = training_data[['full_id', 'text_representation', 'source']]\n",
    "text_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(\n",
    "    '..',\n",
    "    'data',\n",
    "    'training',\n",
    "    'text_representations.csv'\n",
    ")\n",
    "\n",
    "text_reps.to_csv(\n",
    "    output_path,\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stringify lists and explode observations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['tokens'] = training_data['tokens'].progress_apply(stringify_lists)\n",
    "training_data.drop(columns=['text_representation'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.explode('tokens').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'N total observations: {len(training_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(\n",
    "    '..',\n",
    "    'data',\n",
    "    'training',\n",
    "    'training_data.csv'\n",
    ")\n",
    "\n",
    "training_data.to_csv(\n",
    "    output_path,\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<-- Complete -->`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idw-article",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
