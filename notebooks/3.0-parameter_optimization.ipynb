{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP & HDBSCAN Hyperparemeter Tuning\n",
    "\n",
    "This notebook identifies which parameters for **UMAP** and **HDBSCAN** are optimal for topic modeling with BERTopic. The optimization task has two goals: \n",
    "\n",
    "1. Minimize the amount of noise found by HDBSCAN by maximizing the amount of points clustered.\n",
    "2. Produce confident HDBSCAN clusters by minimizing the number of points with low probability of cluster membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.data import load_data\n",
    "from utils.embeddings import load_embeddings\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from functools import partial\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set random seed:\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data & embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /datasets/idw-reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = '/datasets/idw-reddit/training_data.csv'\n",
    "df = load_data(DATA)\n",
    "\n",
    "EMBEDDING_MODEL = 'all-mpnet-base-v2'\n",
    "EMBEDDING_MODEL_PATH = os.path.join('embeddings', f'{EMBEDDING_MODEL}.pickle')\n",
    "embeddings = load_embeddings(EMBEDDING_MODEL_PATH)\n",
    "\n",
    "assert len(embeddings) == len(df), \"Error! Embedding length does not match dataframe length!\"\n",
    "\n",
    "df['embedding'] = list(embeddings)\n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create average embedding representations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average embeddings:\n",
    "print('Averaging embeddings')\n",
    "agg_df = df.groupby(['full_id', 'source'])['embedding'].apply(np.vstack).reset_index()\n",
    "agg_df['embedding'] = agg_df['embedding'].apply(lambda row: row.mean(axis=0))\n",
    "\n",
    "# Aggregate sentences & map to embeddings:\n",
    "print('Mapping aggregated sentences to embeddings')\n",
    "df = df.groupby(['full_id', 'source'])['tokens'].apply(list).reset_index()\n",
    "df['tokens'] = df['tokens'].apply(lambda row: ' '.join(row))\n",
    "agg_df['tokens'] = agg_df['full_id'].map(\n",
    "    dict(\n",
    "        zip(\n",
    "            df['full_id'],\n",
    "            df['tokens']\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# save memory\n",
    "del df\n",
    "\n",
    "# SORT!\n",
    "agg_df.sort_values('full_id', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataframe size: {round(agg_df.memory_usage(deep=True).sum() / 1024**3, 2)} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP & HDBSCAN Fit\n",
    "\n",
    "- Fit a variety of values for the UMAP & HDBSCAN models.\n",
    "- Return the fitted HDBSCAN model for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_hdbscan_clusterer(umap_embedding: np.array,\n",
    "                           min_cluster_size: int,\n",
    "                           min_samples: int) -> HDBSCAN:\n",
    "    \n",
    "    # fit HDBSCAN\n",
    "    hdbscan_model = HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom',\n",
    "        prediction_data=True,\n",
    "        core_dist_n_jobs=mp.cpu_count()-1\n",
    "    )\n",
    "    \n",
    "    clusterer = hdbscan_model.fit(umap_embedding)\n",
    "    \n",
    "    return clusterer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Function\n",
    "\n",
    "- Measure *loss*, defined here as the proportion of points with less than `5%` probability of cluster membership per HDBSCAN.\n",
    "- Also grab the proportion of points assigned as noise (`-1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_func(clusterer: HDBSCAN, threshold: float=0.5) -> tuple:\n",
    "    labels = clusterer.labels_\n",
    "    unique_labels = set(sorted(labels))\n",
    "    \n",
    "    if -1 in unique_labels:\n",
    "        n_labels = len(unique_labels) - 1\n",
    "    else:\n",
    "        n_labels = len(unique_labels)\n",
    "    \n",
    "    probabilites = clusterer.probabilities_\n",
    "    cost = sum([1 for p in probabilites if p < threshold]) / len(labels)\n",
    "    noise_proportion = sum([1 for p in labels if p == -1]) / len(labels)\n",
    "    \n",
    "    return n_labels, cost, noise_proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oobjective Function\n",
    "- Chain `umap_hdbscan_clusterer` and `score_func` together into an objective function.\n",
    "- Add an additional penalty of `0.15` if the number of clusters returned are less than the lower bound of plausible clusters or higher than the higher bound of plausible clusters.\n",
    "    - The goal is to avoid HDBSCAN finding a large amount of \"micro\" clusters that should be part of the same cluster.\n",
    "    - Additionally, we want to avoid HDBSCAN simply lumping points into an implausibly small number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(parameters: dict, umap_embedding: np.array, k_lower: int, k_upper: int) -> dict:\n",
    "    clusterer = umap_hdbscan_clusterer(\n",
    "        umap_embedding=umap_embedding,\n",
    "        min_cluster_size=parameters['min_cluster_size'],\n",
    "        min_samples=parameters['min_samples'],\n",
    "    )\n",
    "    \n",
    "    k_topics, cost, noise_proportion = score_func(clusterer, threshold=0.05)\n",
    "    \n",
    "    # increase penalty for too few and too many clusters:\n",
    "    if (k_topics < k_lower) | (k_topics > k_upper):\n",
    "        penalty = 0.15\n",
    "    else:\n",
    "        penalty = 0.0\n",
    "    \n",
    "    loss = cost + penalty\n",
    "    eval_dict = {\n",
    "        'loss': loss, \n",
    "        'noise_proportion': noise_proportion, \n",
    "        'k_topics': k_topics,\n",
    "        'min_cluster_size': parameters['min_cluster_size'],\n",
    "        'min_samples': parameters['min_samples'],\n",
    "        'status': STATUS_OK\n",
    "    }\n",
    "    \n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperopt Search\n",
    "- Optimize the search using Hyperopt.\n",
    "- The *best* parameters are those that most minimize the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_search(umap_embedding: np.array,\n",
    "                    space: dict,\n",
    "                    k_lower: int,\n",
    "                    k_upper: int,\n",
    "                    max_evals: int=100):\n",
    "    \n",
    "    trials = Trials()\n",
    "    \n",
    "    objective_for_search = partial(\n",
    "        objective_function,\n",
    "        umap_embedding=umap_embedding,\n",
    "        k_lower=k_lower,\n",
    "        k_upper=k_upper\n",
    "    )\n",
    "    \n",
    "    best_fit = fmin(\n",
    "        objective_for_search,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,\n",
    "        trials=trials\n",
    "    )\n",
    "    \n",
    "    hyperparams = space_eval(space, best_fit)\n",
    "    print(f'Best fit: {hyperparams}')\n",
    "    print(f'K topics: {trials.best_trial[\"result\"][\"k_topics\"]}')\n",
    "    \n",
    "    return best_fit, hyperparams, trials\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Optimizer\n",
    "\n",
    "- Establish the search space of UMAP & HDBSCAN parameters.\n",
    "- Iterate over the search space using Hyperopt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(\n",
    "    n_components=5,\n",
    "    n_neighbors=15,\n",
    "    metric='cosine',\n",
    "    min_dist=0.0,\n",
    "    init='tswspectral',\n",
    "    unique=True,\n",
    "    n_epochs=400,\n",
    "    low_memory=True,\n",
    "    random_state=137,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "umap_embedding = umap_model.fit_transform(np.array(agg_df['embedding'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'min_cluster_size': hp.choice('min_cluster_size', range(50,200+1,1)),\n",
    "    'min_samples': hp.choice('min_samples', range(2,50+1,1))\n",
    "}\n",
    "\n",
    "k_lower = 50\n",
    "k_upper = 300\n",
    "max_evals = 100\n",
    "\n",
    "best_params, hyperparams, trials = hyperopt_search(\n",
    "    umap_embedding=umap_embedding,\n",
    "    space=search_space,\n",
    "    k_lower=k_lower,\n",
    "    k_upper=k_upper,\n",
    "    max_evals=max_evals\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_eval(search_space, best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Fits\n",
    "- Iterate over the Hyperopt trials and obtain the performance across parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fits = []\n",
    "for trial in trials.trials:\n",
    "    results = trial['result']\n",
    "\n",
    "    payload = {\n",
    "        'loss': results['loss'],\n",
    "        'min_cluster_size': results['min_cluster_size'],\n",
    "        'min_samples': results['min_samples'],\n",
    "        'noise_proportion': results['noise_proportion'],\n",
    "        'k_topics': results['k_topics']\n",
    "    }\n",
    "\n",
    "    model_fits.append(payload)\n",
    "\n",
    "model_fits = pd.DataFrame(model_fits).sort_values('loss', ascending=True).reset_index(drop=True)\n",
    "model_fits.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fit data:\n",
    "print('Saving model fits data')\n",
    "model_fits.to_csv(\n",
    "    os.path.join(\n",
    "        'model_fits',\n",
    "        'model_fits.csv'\n",
    "    ),\n",
    "    index=False\n",
    ")\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
