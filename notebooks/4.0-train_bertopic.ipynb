{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50af7eb-ba97-4299-a55a-fe952346fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "\n",
    "from utils.data import load_data\n",
    "from utils.embeddings import load_embeddings\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# set random seed:\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896ecc1-235d-481c-acde-adeeb8ad76fe",
   "metadata": {},
   "source": [
    "**Load data & embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6bcc3-ab79-4580-9a8e-c733a3af0217",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /datasets/idw-reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eeb5d8-1655-4c91-8529-55f8a0ca72ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = '/datasets/idw-reddit/training_data.csv'\n",
    "df = load_data(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065357bb-71ac-492f-a00c-82b9c771928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = 'all-mpnet-base-v2'\n",
    "EMBEDDING_MODEL_PATH = os.path.join('embeddings', f'{EMBEDDING_MODEL}.pickle')\n",
    "embeddings = load_embeddings(EMBEDDING_MODEL_PATH)\n",
    "\n",
    "assert len(embeddings) == len(df), \"Error! Embedding length does not match dataframe length!\"\n",
    "\n",
    "df['embedding'] = list(embeddings)\n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f19d08-25d0-4dfd-b975-142474ebb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4365292e-0eb4-4900-821d-e44e885ab966",
   "metadata": {},
   "source": [
    "**Create average embedding representations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152f1ed-133e-40f0-9896-37e61a7003e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average embeddings:\n",
    "print('Averaging embeddings')\n",
    "agg_df = df.groupby(['full_id', 'source'])['embedding'].apply(np.vstack).reset_index()\n",
    "agg_df['embedding'] = agg_df['embedding'].apply(lambda row: row.mean(axis=0))\n",
    "\n",
    "# Aggregate sentences & map to embeddings:\n",
    "print('Mapping aggregated sentences to embeddings')\n",
    "df = df.groupby(['full_id', 'source'])['tokens'].apply(list).reset_index()\n",
    "df['tokens'] = df['tokens'].apply(lambda row: ' '.join(row))\n",
    "agg_df['tokens'] = agg_df['full_id'].map(\n",
    "    dict(\n",
    "        zip(\n",
    "            df['full_id'],\n",
    "            df['tokens']\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# save memory\n",
    "del df\n",
    "\n",
    "# SORT!\n",
    "agg_df.sort_values('full_id', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664654c-6408-42be-bfae-822d09c427ab",
   "metadata": {},
   "source": [
    "**Map text representations back to dataframe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e547aac-f843-44b4-9f3f-99bf955b0a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_PATH = '/datasets/idw-reddit/text_representations.csv'\n",
    "text_reps = pd.read_csv(TEXT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74edb67-6402-4290-91d5-8872f68bfa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_reps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81441d10-9ff2-4ad7-b0cd-f7a6589a5032",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df['text_representation'] = agg_df['full_id'].map(\n",
    "    dict(\n",
    "        zip(\n",
    "            text_reps['full_id'],\n",
    "            text_reps['text_representation']\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d9d60-da16-4ac1-bdad-d5cad3cb45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256379b8-84bc-4ce0-9b7c-b7e493ab2a59",
   "metadata": {},
   "source": [
    "**Save embeddings mapped to full_ids:**\n",
    "- This ensures no issues with re-arranged rows in the Pandas `groupby`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec92590-e715-4e77-931c-3a8b2d28c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ids = agg_df['full_id'].tolist()\n",
    "arrays_to_list = [arr.tolist() for arr in tqdm(agg_df['embedding'].tolist())]\n",
    "out_arrs = [{'full_id': k, 'embeddings': v} for k,v in zip(full_ids, arrays_to_list)]\n",
    "\n",
    "with open(f'embeddings/full_id_to_embeddings.jsonl', 'w') as f:\n",
    "    for arr in tqdm(out_arrs):\n",
    "        f.write(json.dumps(arr) + '\\n')\n",
    "                                               \n",
    "del full_ids, arrays_to_list, out_arrs\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c842b4c-b37c-4896-9429-9389f67dcc61",
   "metadata": {},
   "source": [
    "## Train BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e91c2-1b38-4d8d-aca9-714d36d36301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "umap_model = UMAP(\n",
    "    n_components=5,\n",
    "    n_neighbors=15,\n",
    "    metric='cosine',\n",
    "    min_dist=0.0,\n",
    "    init='tswspectral',\n",
    "    unique=True,\n",
    "    n_epochs=400,\n",
    "    low_memory=True,\n",
    "    random_state=137,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# HDBSCAN\n",
    "min_cluster_size = 196\n",
    "min_samples = 34\n",
    "cluster_method = 'eom'\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    min_samples=min_samples,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method=cluster_method,\n",
    "    prediction_data=True,\n",
    "    core_dist_n_jobs=mp.cpu_count()-1\n",
    ")\n",
    "\n",
    "# CountVectorizer\n",
    "min_gram = 1\n",
    "max_gram = 1\n",
    "\n",
    "cv_model = CountVectorizer(\n",
    "    min_df=1,\n",
    "    max_df=0.95,\n",
    "    stop_words=list(\n",
    "        stopwords.words('english')\n",
    "    ),\n",
    "    ngram_range=(min_gram, max_gram)\n",
    ")\n",
    "\n",
    "# MMR for diversity:\n",
    "diversity = 0.3\n",
    "mmr_model = MaximalMarginalRelevance(diversity=diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25556e2-b724-465f-a271-1b483cf5d9ed",
   "metadata": {},
   "source": [
    "## Fit BERTopic Model\n",
    "- Save model to `fit_bertopic_model` in the `models` directory.\n",
    "- Get topics and save them to `fit_data.csv` in the `models` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a632dc-efa1-45f3-b68b-7bfed64f9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=cv_model,\n",
    "    calculate_probabilities=True,\n",
    "    top_n_words=20,\n",
    "    representation_model=mmr_model\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(\n",
    "    agg_df['text_representation'].tolist(),\n",
    "    np.array(agg_df['embedding'].tolist())\n",
    ")\n",
    "\n",
    "# save model:\n",
    "MODEL_NAME = 'fit_bertopic_model'\n",
    "MODEL_PATH = os.path.join('models', MODEL_NAME)\n",
    "\n",
    "print(f'Saving model to {MODEL_PATH}')\n",
    "topic_model.save(MODEL_PATH, serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49481ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef7621e-dc99-4c82-b71f-bf196b0f951b",
   "metadata": {},
   "source": [
    "`---Complete---`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
